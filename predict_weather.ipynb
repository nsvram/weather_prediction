{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Using cached geopy-1.12.0-py2.py3-none-any.whl\n",
      "Installing collected packages: geopy\n",
      "Successfully installed geopy-1.12.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 9.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from geopy.distance import vincenty\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql import functions as f, SparkSession,SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext('local[*]')\n",
    "spark      =  SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_conditions(row):\n",
    "    #Function to caluculate and predict the day as Rain, Snow, Sunny, etc\n",
    "    if row['rainfall_mm'] >= 1.5:\n",
    "        return 'Rain'\n",
    "    elif row['minimum_temperature'] <= 0 and row['maximum_temperature'] <= 10:\n",
    "        return 'Snow' \n",
    "    elif row['maximum_temperature'] >= 30 and row['minimum_temperature'] <= 25:\n",
    "        return 'Sunny' \n",
    "    else:\n",
    "        return 'Moderate'\n",
    "\n",
    "def date_rolling_range(no_of_days,date_to_predict):\n",
    "    # Returns a list of dates with specific format\n",
    "    return [(datetime.strptime(date_to_predict, '%Y-%m-%d') - timedelta(days=365) - timedelta(days=no_of_days)).strftime('%Y-%m-%d'), \n",
    "            (datetime.strptime(date_to_predict, '%Y-%m-%d') - timedelta(days=365) + timedelta(days=no_of_days)).strftime('%Y-%m-%d')]\n",
    "\n",
    "def mean_per_date_station(df,date_range, in_station_id):\n",
    "    # return mean per station id\n",
    "    filter_range = (df.station_id == in_station_id)&(df.date_formated >= date_range[0]) & (df.date_formated <= date_range[1])\n",
    "    return df.loc[filter_range].groupby(['station_id']).mean()\n",
    "\n",
    "def predict_weather(station_id, date_to_predict):\n",
    "    # Used Grand Mean method to predict weather\n",
    "    # https://en.wikipedia.org/wiki/Grand_mean\n",
    "    history_weather = pd.read_csv(\"/home/jovyan/data/weather_extract.csv\", index_col=\"date\")\n",
    "    df_station_list = history_weather[['station_id','station_name']].groupby(['station_id','station_name']).count()\n",
    "    history_weather['date_formated']=history_weather.index\n",
    "    history_weather.date_formated = history_weather.date_formated.apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "    union_df = pd.concat([mean_per_date_station(history_weather,date_rolling_range(3,date_to_predict),station_id)\n",
    "                          ,mean_per_date_station(history_weather,date_rolling_range(15,date_to_predict),station_id)\n",
    "                          ,mean_per_date_station(history_weather,date_rolling_range(30,date_to_predict),station_id)])\n",
    "    final_prediction = union_df.groupby(union_df.index).mean()\n",
    "    final_prediction_pred =  final_prediction[['Lat', 'Lon','rainfall_mm','3pm_relative_humidity','3pm_msl_pressure_hpa', 'minimum_temperature', 'maximum_temperature']]\n",
    "    final_prediction_pred[\"Conditions\"] = final_prediction_pred.apply(cal_conditions,axis=1)\n",
    "    final_prediction_pred[\"predicted_date\"] = date_to_predict\n",
    "    final_prediction_pred.join(df_station_list)\n",
    "    df_station_list.to_csv(\"staion_id.csv\")\n",
    "    with open('/home/jovyan/data/prediction_weather.psv', 'a') as f:\n",
    "        final_prediction_pred.to_csv(f, header=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function all runs the predict_weather function from provided date and number of days\n",
    "start_time = datetime.now()\n",
    "strat_date = '2018-01-01'\n",
    "no_of_days = 365\n",
    "station_id_list = [3003, 9021, 9999, 12038,18192,23090,24048,26021,31011]\n",
    "date_list = [(datetime.strptime(strat_date, '%Y-%m-%d') + timedelta(days=x)).strftime('%Y-%m-%d')\n",
    "             for x in range(0, no_of_days)]\n",
    "for day in date_list:\n",
    "    for station in station_id_list:\n",
    "        predict_weather(station, day)\n",
    "        #print ((\"Predicting Weather for the date - {0} and weather station id - {1}\").format(day, station))\n",
    "end_time = datetime.now()  \n",
    "time_delta = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run the prediction - 0:09:30\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken to run the prediction - \" + str(timedelta(seconds=time_delta.seconds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geospatial calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_airport = pd.read_csv(\"/home/jovyan/data/aus_airport.csv\")\n",
    "col = ['staion_id','Lat', 'Lon','rainfall_mm','3pm_relative_humidity','3pm_msl_pressure_hpa', 'minimum_temperature', 'maximum_temperature', 'Condition', 'date']\n",
    "prediction_weather_staion_id = pd.read_csv(\"/home/jovyan/data/prediction_weather.psv\", header=None,sep='|')\n",
    "prediction_weather_staion_id.columns = col\n",
    "df_prediction_weather_staion = spark.createDataFrame(prediction_weather_staion_id)\n",
    "weather_stations = prediction_weather_staion_id[['staion_id','Lat', 'Lon']].drop_duplicates()\n",
    "df_prediction_weather_staion.registerTempTable(\"df_prediction_weather_staion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stations['station_coords'] = tuple(zip(weather_stations['Lat'], weather_stations['Lon']))\n",
    "df_airport['airport_coords'] = tuple(zip(df_airport['Latitude'],df_airport['Longitude']))\n",
    "weather_stations.reset_index(drop=True)\n",
    "weather_stations.index = range(len(weather_stations.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_lst_station_airport = []\n",
    "for i in df_airport.index.values:\n",
    "    for j in weather_stations.index.values:\n",
    "        my_lst_station_airport.append([weather_stations.iloc[j][0] , \n",
    "               df_airport.iloc[i][0] , \n",
    "               df_airport.iloc[i][1] ,\n",
    "               (vincenty(weather_stations.iloc[j][3], df_airport.iloc[i][4]).km)])\n",
    "\n",
    "df_station_airport = pd.DataFrame(my_lst_station_airport)\n",
    "df_station_airport.columns = ['station_id', 'Airport_name', 'IATA_code','Distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = spark.createDataFrame(df_station_airport)\n",
    "df_1.registerTempTable(\"df_1\")\n",
    "df_2 = spark.sql(\"\"\"\n",
    "SELECT \n",
    "station_id,\n",
    "Airport_name,\n",
    "IATA_code,Distance,\n",
    "ROW_NUMBER() OVER (PARTITION BY \n",
    "                            IATA_code\n",
    "                            ORDER BY \n",
    "                            Distance) AS rnA\n",
    "FROM \n",
    "df_1\n",
    "\"\"\")\n",
    "df_3 = df_2.filter(\"rnA =1\")\n",
    "df_3.registerTempTable(\"df_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_psv_output = spark.sql(\"\"\"\n",
    "SELECT \n",
    " IATA_code as Station\n",
    ",Lat\n",
    ",Lon\n",
    ",date as  Local_Time\n",
    ",Condition\n",
    ",maximum_temperature as Temperature\n",
    ",3pm_msl_pressure_hpa as Pressure \n",
    ",3pm_relative_humidity as Humidity\n",
    "\n",
    "FROM \n",
    "df_3 A Inner join df_prediction_weather_staion B on a.station_id = B.staion_id\n",
    "order by date\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_psv_output.write.csv('data/IATA_weather_prediction.psv',sep = '|', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
